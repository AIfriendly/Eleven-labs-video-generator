"""
Extended Unit Tests for UsageMonitor and UsageDisplay (Automation Expansion).

This file contains additional tests to expand coverage for:
- Thread-safety of UsageMonitor (concurrent access)
- PricingStrategy edge cases
- UsageDisplay live update lifecycle
- Error boundaries and defensive programming

Generated by testarch-automate workflow for Story 5.5.
"""
import pytest
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from unittest.mock import MagicMock, patch, PropertyMock

from eleven_video.monitoring.usage import (
    UsageMonitor,
    PricingStrategy,
    UsageEvent,
    MetricType,
    METRIC_INPUT_TOKENS,
    METRIC_OUTPUT_TOKENS,
    METRIC_CHARACTERS,
    METRIC_IMAGES,
)
from eleven_video.ui.usage_panel import UsageDisplay


# =============================================================================
# Fixtures
# =============================================================================


@pytest.fixture(autouse=True)
def clean_monitor_state():
    """Reset UsageMonitor and PricingStrategy state before and after each test.
    
    This fixture ensures test isolation by:
    - Resetting pricing to defaults before each test
    - Clearing all tracked usage events before each test
    - Cleaning up after the test completes (even if it fails)
    """
    # Setup: Reset to clean state
    PricingStrategy.reset()
    monitor = UsageMonitor.get_instance()
    monitor.reset()
    
    yield monitor
    
    # Teardown: Always reset after test (runs even if test fails)
    PricingStrategy.reset()
    monitor.reset()


# =============================================================================
# P0 Thread-Safety Tests - Concurrent Access (Risk R-002)
# =============================================================================


class TestThreadSafety:
    """[AUTO-001] Tests for UsageMonitor thread-safety under concurrent access."""

    def test_concurrent_track_usage_calls(self, clean_monitor_state):
        """[AUTO-001a][P0] Verify concurrent track_usage calls don't corrupt data.
        
        GIVEN: Multiple threads tracking usage simultaneously
        WHEN: 100 concurrent track_usage calls are made
        THEN: All events are recorded correctly without data corruption
        
        Risk: R-002 (Thread-safety)
        """
        # GIVEN: Monitor ready for tracking
        monitor = clean_monitor_state
        num_threads = 10
        calls_per_thread = 10
        expected_total_calls = num_threads * calls_per_thread
        
        def track_usage_task(thread_id: int):
            """Task to track usage from a specific thread."""
            for i in range(calls_per_thread):
                monitor.track_usage(
                    service="gemini",
                    model_id=f"model-thread-{thread_id}",
                    metric_type=METRIC_INPUT_TOKENS,
                    value=1000
                )
            return thread_id
        
        # WHEN: Execute concurrent track_usage calls
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = [executor.submit(track_usage_task, i) for i in range(num_threads)]
            for future in as_completed(futures):
                future.result()  # Ensure all complete successfully
        
        # THEN: All events recorded correctly
        events = monitor.get_events()
        assert len(events) == expected_total_calls, (
            f"Expected {expected_total_calls} events, got {len(events)}"
        )

    def test_concurrent_get_summary_during_tracking(self, clean_monitor_state):
        """[AUTO-001b][P0] Verify get_summary is thread-safe during active tracking.
        
        GIVEN: One thread tracking usage
        WHEN: Another thread calls get_summary repeatedly
        THEN: No exceptions are raised and summary is always valid
        
        Risk: R-002 (Thread-safety)
        """
        # GIVEN: Monitor ready
        monitor = clean_monitor_state
        stop_event = threading.Event()
        errors = []
        
        def tracker_thread():
            """Track usage events continuously."""
            for i in range(50):
                if stop_event.is_set():
                    break
                monitor.track_usage(
                    service="gemini",
                    model_id="gemini-2.5-flash",
                    metric_type=METRIC_INPUT_TOKENS,
                    value=100
                )
                time.sleep(0.001)
        
        def reader_thread():
            """Read summary continuously."""
            for i in range(50):
                if stop_event.is_set():
                    break
                try:
                    summary = monitor.get_summary()
                    # Summary must always have required keys
                    assert "total_cost" in summary
                    assert "by_service" in summary
                    assert "by_model" in summary
                except Exception as e:
                    errors.append(str(e))
                time.sleep(0.001)
        
        # WHEN: Run both threads concurrently
        t1 = threading.Thread(target=tracker_thread)
        t2 = threading.Thread(target=reader_thread)
        t1.start()
        t2.start()
        t1.join(timeout=5)
        t2.join(timeout=5)
        stop_event.set()
        
        # THEN: No errors during concurrent access
        assert len(errors) == 0, f"Thread-safety errors: {errors}"

    def test_singleton_thread_safety(self):
        """[AUTO-001c][P1] Verify singleton pattern is thread-safe.
        
        GIVEN: Multiple threads requesting UsageMonitor.get_instance()
        WHEN: All requests complete
        THEN: All threads receive the same instance
        """
        # GIVEN: Container for instances
        instances = []
        lock = threading.Lock()
        
        def get_instance_task():
            instance = UsageMonitor.get_instance()
            with lock:
                instances.append(instance)
        
        # WHEN: Request instance from multiple threads
        threads = [threading.Thread(target=get_instance_task) for _ in range(10)]
        for t in threads:
            t.start()
        for t in threads:
            t.join(timeout=2)
        
        # THEN: All instances are the same object
        assert len(instances) == 10
        assert all(inst is instances[0] for inst in instances)


# =============================================================================
# P1 PricingStrategy Edge Cases
# =============================================================================


class TestPricingStrategyEdgeCases:
    """[AUTO-002] Tests for PricingStrategy edge cases and defensive programming."""

    def test_unknown_service_returns_zero(self, clean_monitor_state):
        """[AUTO-002a][P1] Verify unknown service returns 0 price.
        
        GIVEN: A service not in defaults or overrides
        WHEN: get_price() is called for that service
        THEN: Returns 0.0 (no crash)
        """
        # WHEN: Get price for unknown service
        price = PricingStrategy.get_price("unknown_service", "input_token_price_per_million")
        
        # THEN: Returns 0
        assert price == 0.0

    def test_unknown_price_key_returns_zero(self, clean_monitor_state):
        """[AUTO-002b][P1] Verify unknown price key returns 0 price.
        
        GIVEN: A valid service but invalid price key
        WHEN: get_price() is called with unknown key
        THEN: Returns 0.0 (no crash)
        """
        # WHEN: Get price for unknown key
        price = PricingStrategy.get_price("gemini", "unknown_key")
        
        # THEN: Returns 0
        assert price == 0.0

    def test_partial_override_preserves_defaults(self, clean_monitor_state):
        """[AUTO-002c][P1] Verify partial override doesn't affect other defaults.
        
        GIVEN: Custom override for one Gemini price
        WHEN: Other Gemini prices are queried
        THEN: Default values are still used for un-overridden keys
        """
        # GIVEN: Override only input token price
        PricingStrategy.configure({
            "gemini": {
                "input_token_price_per_million": 999.99
            }
        })
        
        # THEN: Overridden price uses new value
        assert PricingStrategy.get_price("gemini", "input_token_price_per_million") == 999.99
        
        # AND: Non-overridden prices use defaults
        output_price = PricingStrategy.get_price("gemini", "output_token_price_per_million")
        assert output_price == 1.50  # Default value

    def test_multiple_configure_calls_replace(self, clean_monitor_state):
        """[AUTO-002d][P2] Verify multiple configure calls replace (not merge) overrides.
        
        GIVEN: First configure sets input_token price
        WHEN: Second configure sets only output_token price  
        THEN: First override is replaced (input_token reverts to default)
        
        Note: This documents the ACTUAL behavior of PricingStrategy.configure()
        """
        # GIVEN: First configuration
        PricingStrategy.configure({
            "gemini": {"input_token_price_per_million": 1.00}
        })
        assert PricingStrategy.get_price("gemini", "input_token_price_per_million") == 1.00
        
        # WHEN: Second configuration (different key) - REPLACES first
        PricingStrategy.configure({
            "gemini": {"output_token_price_per_million": 3.00}
        })
        
        # THEN: Second is active, first is gone (back to default)
        assert PricingStrategy.get_price("gemini", "output_token_price_per_million") == 3.00
        # Input token price reverts to default (0.50) since configure replaces
        assert PricingStrategy.get_price("gemini", "input_token_price_per_million") == 0.50



# =============================================================================
# P1 UsageDisplay Lifecycle Tests
# =============================================================================


class TestUsageDisplayLifecycle:
    """[AUTO-003] Tests for UsageDisplay start/stop lifecycle."""

    def test_stop_before_start_is_safe(self, clean_monitor_state):
        """[AUTO-003a][P1] Verify stop_live_update before start doesn't crash.
        
        GIVEN: A new UsageDisplay instance
        WHEN: stop_live_update is called without starting
        THEN: No exception is raised
        """
        # GIVEN: New display
        display = UsageDisplay()
        
        # WHEN/THEN: Stop without start - no crash
        display.stop_live_update()  # Should not raise
        assert display._update_thread is None

    def test_double_start_is_idempotent(self, clean_monitor_state):
        """[AUTO-003b][P1] Verify double start doesn't create multiple threads.
        
        GIVEN: UsageDisplay with live update started
        WHEN: start_live_update is called again
        THEN: Only one update thread exists
        """
        # GIVEN: Display with mocked console to avoid actual rendering
        display = UsageDisplay()
        mock_console = MagicMock()
        
        # Mock Live context to avoid threading issues in test
        with patch("eleven_video.ui.usage_panel.Live"):
            # We can't fully test threaded behavior without timing issues
            # Instead verify the guard clause exists
            display._update_thread = MagicMock()
            display._update_thread.is_alive.return_value = True
            
            # WHEN: Try to start again
            # THEN: Should return early (no new thread created)
            display.start_live_update(mock_console)
            # The guard returns early, so no exception

    def test_render_once_with_no_data(self, clean_monitor_state):
        """[AUTO-003c][P1] Verify render_once works with empty monitor.
        
        GIVEN: Empty UsageMonitor (no events tracked)
        WHEN: render_once is called
        THEN: Renders without crash, shows $0.00 total
        """
        # GIVEN: Clean monitor (no events)
        display = UsageDisplay()
        mock_console = MagicMock()
        
        # WHEN: Render once
        display.render_once(mock_console)
        
        # THEN: Console.print was called with the display
        mock_console.print.assert_called_once_with(display)


# =============================================================================
# P2 Boundary Value Tests
# =============================================================================


class TestBoundaryValues:
    """[AUTO-004] Tests for boundary values and edge cases."""

    def test_zero_value_tracking(self, clean_monitor_state):
        """[AUTO-004a][P2] Verify zero values are tracked correctly.
        
        GIVEN: Zero tokens tracked
        WHEN: Summary is requested
        THEN: Zero cost is calculated
        """
        # GIVEN: Clean monitor
        monitor = clean_monitor_state
        
        # WHEN: Track zero tokens
        monitor.track_usage(
            service="gemini",
            model_id="gemini-2.5-flash",
            metric_type=METRIC_INPUT_TOKENS,
            value=0
        )
        
        # THEN: Summary shows zero cost
        summary = monitor.get_summary()
        assert summary["total_cost"] == 0.0
        assert summary["events_count"] == 1  # Event still recorded

    def test_large_value_tracking(self, clean_monitor_state):
        """[AUTO-004b][P2] Verify large token counts are handled correctly.
        
        GIVEN: Very large token count (10 billion)
        WHEN: Cost is calculated
        THEN: Math doesn't overflow and cost is correct
        """
        # GIVEN: Clean monitor
        monitor = clean_monitor_state
        large_value = 10_000_000_000  # 10 billion tokens
        
        # WHEN: Track large value
        monitor.track_usage(
            service="gemini",
            model_id="gemini-2.5-flash",
            metric_type=METRIC_INPUT_TOKENS,
            value=large_value
        )
        
        # THEN: Cost calculated correctly (10B tokens * $0.50/M = $5000)
        summary = monitor.get_summary()
        expected_cost = (large_value / 1_000_000) * 0.50
        assert summary["total_cost"] == pytest.approx(expected_cost, rel=0.0001)

    def test_negative_value_handled(self, clean_monitor_state):
        """[AUTO-004c][P2] Verify negative values don't crash (defensive).
        
        GIVEN: Negative token count (invalid but shouldn't crash)
        WHEN: Cost is calculated
        THEN: No crash, result may be negative cost (acceptable edge case)
        """
        # GIVEN: Clean monitor
        monitor = clean_monitor_state
        
        # WHEN: Track negative value (edge case)
        monitor.track_usage(
            service="gemini",
            model_id="gemini-2.5-flash",
            metric_type=METRIC_INPUT_TOKENS,
            value=-1000
        )
        
        # THEN: No crash - calculates (may result in negative cost)
        summary = monitor.get_summary()
        assert "total_cost" in summary  # Key exists, no crash


# =============================================================================
# P2 MetricType Enum Tests
# =============================================================================


class TestMetricTypes:
    """[AUTO-005] Tests for all MetricType values."""

    def test_all_metric_types_have_values(self):
        """[AUTO-005a][P2] Verify all MetricType enum values are strings.
        
        GIVEN: MetricType enum
        WHEN: All values are inspected
        THEN: Each has a non-empty string value
        """
        # THEN: All expected values exist
        assert MetricType.INPUT_TOKENS == "input_tokens"
        assert MetricType.OUTPUT_TOKENS == "output_tokens"
        assert MetricType.CHARACTERS == "characters"
        assert MetricType.IMAGES == "images"

    def test_constants_match_enum(self):
        """[AUTO-005b][P2] Verify module constants match MetricType enum.
        
        GIVEN: Module-level METRIC_* constants
        WHEN: Compared to MetricType enum
        THEN: Values match exactly
        """
        assert METRIC_INPUT_TOKENS == MetricType.INPUT_TOKENS
        assert METRIC_OUTPUT_TOKENS == MetricType.OUTPUT_TOKENS
        assert METRIC_CHARACTERS == MetricType.CHARACTERS
        assert METRIC_IMAGES == MetricType.IMAGES
